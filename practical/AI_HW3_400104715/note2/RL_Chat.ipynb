{"cells":[{"cell_type":"markdown","metadata":{"id":"sWZ3HloS1Ok-"},"source":["\u003cimg src='http://www-scf.usc.edu/~ghasemig/images/sharif.png' alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\" \u003e\n","\n","\u003cbr\u003e\n","\u003cfont\u003e\n","\u003cdiv dir=ltr align=center\u003e\n","\u003cfont color=0F5298 size=7\u003e\n","    Artificial Intelligence \u003cbr\u003e\n","\u003cfont color=2565AE size=5\u003e\n","    Computer Engineering Department \u003cbr\u003e\n","    Spring 2023\u003cbr\u003e\n","\u003cfont color=3C99D size=5\u003e\n","    Practical Assignment 3 - Reinforcement Learning \u003cbr\u003e\n","\u003cfont color=696880 size=4\u003e\n","    Mohammad Moshtaghi - Ali Salesi - Hossein Goli\n","\n","____"]},{"cell_type":"markdown","metadata":{"id":"ejfGdour1cNK"},"source":["# Personal Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IM62bqV51dy2"},"outputs":[],"source":["student_number = '400104715'\n","first_name = 'Ali'\n","last_name = 'Aghayari'"]},{"cell_type":"markdown","metadata":{"id":"l3KLkyuZo0tR"},"source":["# Rules\n","- Make sure that all of your cells can be run perfectly. "]},{"cell_type":"markdown","metadata":{"id":"z91za1kfo7uB"},"source":["# Q2: Sentence Generator (100 Points)"]},{"cell_type":"markdown","metadata":{"id":"H14bE9pnA_I7"},"source":["\u003cfont size=4\u003e\n","Author: Ali Salesi\n","\u003cbr/\u003e\n","\u003cfont color=red\u003e\n","Please run all the cells.\n","\u003c/font\u003e\n","\u003c/font\u003e\n","\u003cbr/\u003e\n","\u003c/div\u003e"]},{"cell_type":"markdown","metadata":{"id":"R2vBT4rxeHnM"},"source":["In this assignment we implement a text generator using RL."]},{"cell_type":"markdown","metadata":{"id":"gQRBpSNJ2ICr"},"source":["## Preprocess"]},{"cell_type":"markdown","metadata":{"id":"KDg8VW5k3A4Z"},"source":["### Dataset"]},{"cell_type":"markdown","metadata":{"id":"ylFoOb7GIRI3"},"source":["First, lets download the text corpus crawled from `VOA Persian` from 2003 to 2008."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1682,"status":"ok","timestamp":1684081753727,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"mxG1ZP8Gqk2_","outputId":"a710d5ad-4297-4d19-9307-ac3fe6ee2a85"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-05-14 16:29:11--  https://storage.googleapis.com/danielk-files/farsi-text/merged_files/voa_persian_2003_2008_cleaned.txt\n","Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.2.128, 142.250.141.128, 2607:f8b0:4023:c0d::80, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.2.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 69708061 (66M) [text/plain]\n","Saving to: ‘voa_persian.txt’\n","\n","voa_persian.txt     100%[===================\u003e]  66.48M   161MB/s    in 0.4s    \n","\n","2023-05-14 16:29:12 (161 MB/s) - ‘voa_persian.txt’ saved [69708061/69708061]\n","\n"]}],"source":["!wget -O \"voa_persian.txt\" \"https://storage.googleapis.com/danielk-files/farsi-text/merged_files/voa_persian_2003_2008_cleaned.txt\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1684081753728,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"Mzh2UPwisjTG","outputId":"15b75054-e7dc-454d-b176-bdf93e28aa0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["488253\n"]}],"source":["!wc -l voa_persian.txt | awk '{print $1}'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1684081753728,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"xDKxtmaAqvjB","outputId":"1f9a6b4f-63f8-4ca4-dc82-bcc2c742acc8"},"outputs":[{"name":"stdout","output_type":"stream","text":["پيمان صلح بين ژاپن و روسيه\n","بنا به گزارشهای منتشره در توکيو، ژاپن و روسيه در زمينه يک پيمان صلح در چارچوبی گسترده توافق کرده اند که رسماً به مخاصمات جنگ دوم جهانی ميان دو کشور پايان خواهند داد.\n","\n","در يکی از اين گزارشها، که از سوی خبرگزاری کيودُو،انتشار يافته، گفته شده است که دو کشور برای رفع اختلافات ديرين خود بر سر چهار جزيره از جزاير زنجيره ای کوريل، بر اساس سه پيمان گذشته خود عمل خواهند کرد.\n","بموجب يکی از اين پيمانها که در سال ۱۹۵۶ امضاء شده، دو تا از اين جزيره ها پس از امضاء يک پيمان صلح به ژاپن پس داده خواهد شد.\n","اما بموجب پيمانی که در سال ۱۹۹۳ به امضاء رسيده، مسئله حاکميت اين چهار جزيره بايستی پيش از امضاء پيمان صلح فيصله يابد.\n","هيچ يک از دو طرف نحوه استفاده از پيمان های پيشين را اعلام نکرده اند.\n","\n","تشکيلات فلسطينی نخستين بودجه رسمی خود را اعلام کرد\n","تشکيلات فلسطينی پس از دو سال نخستين بودجه رسمی خود را اعلام کرد و قول داد برای از ميان برداشتن فساد و پاسخگوئی بيشتر به مردم تلاش کند.\n"]}],"source":["!head voa_persian.txt"]},{"cell_type":"markdown","metadata":{"id":"cj9v5jUm2691"},"source":["### Normalization"]},{"cell_type":"markdown","metadata":{"id":"Zx5YwlZar85k"},"source":["Then we have to normalize and lemmatize the text so we can have a better generalization of semantics in prompt generation.\n","\n","We'll use `hazm` library for this purpose."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5853,"status":"ok","timestamp":1684081759570,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"YWwihzn7rzAC"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: hazm in /usr/local/lib/python3.10/dist-packages (0.7.0)\n","Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.10/dist-packages (from hazm) (3.3)\n","Requirement already satisfied: libwapiti\u003e=0.2.1 in /usr/local/lib/python3.10/dist-packages (from hazm) (0.2.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.3-\u003ehazm) (1.16.0)\n"]}],"source":["!pip install hazm"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"c_qV0iYsr-lB"},"outputs":[],"source":["from __future__ import unicode_literals\n","from hazm import Normalizer, Lemmatizer, word_tokenize\n","from tqdm import tqdm\n","import re\n","\n","normalizer = Normalizer()\n","lemmatizer = Lemmatizer()\n","\n","\n","def normalize(line: str):\n","    line = re.sub(\n","        r'[.{}[\\]؛:«»؟!٬٫٪×،*)(ـ+\u003c\u003e\\'\",`=+\\-?!@#$%^\u0026*()_\\/\\\\\\\\]', '', line.strip())\n","    line = re.sub(r'\\s+', ' ', line.strip())\n","    line = normalizer.normalize(line)\n","    words = word_tokenize(line)\n","    words = [lemmatizer.lemmatize(word) for word in words]\n","    line = ' '.join(words)\n","    return line\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1684081759574,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"bJwjSKcs1eEf"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'من خیلی خوشحال #هست و کتاب زیاد درباره یخچال قطب خواند#خوان'"]},"execution_count":75,"metadata":{},"output_type":"execute_result"}],"source":["normalize('من خیلی خوشحال هستم و کتاب‌های زیادی درباره یخچال‌های قطبی خوانده‌ام.')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64445,"status":"ok","timestamp":1684081824005,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"AOD9fyM3sK8K"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 488253/488253 [01:02\u003c00:00, 7836.79it/s]\n"]}],"source":["voa = open('voa_persian.txt')\n","voa_norm = open('voa_persian_normalized.txt', 'w')\n","for i, line in tqdm(enumerate(voa), total=488253):\n","    voa_norm.write(normalize(line) + '\\n')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":679,"status":"ok","timestamp":1684081824667,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"Df4v9sYDxDT9"},"outputs":[{"name":"stdout","output_type":"stream","text":["پیمان صلح بین ژاپن و روسیه\n","بنا به گزارش منتشره در توکیو ژاپن و روسیه در زمینه یک پیمان صلح در چارچوب گسترده توافق کرد#کن که رسما به مخاصمات جنگ دوم جهانی میان دو کشور پایان داد#ده\n","\n","در یک از این گزارش که از سو خبرگزاری کیودوانتشار یافته گفت#گو که دو کشور برای رفع اختلافات دیرین خود بر سر چهار جزیره از جزایر زنجیره کوریل بر اساس سه پیمان گذشته خود عمل کرد#کن\n","بموجب یک از این پیمان که در سال ۱۹۵۶ امضاء شده دو تا از این جزیره پس از امضاء یک پیمان صلح به ژاپن پس داد#ده\n","اما بموجب پیمان که در سال ۱۹۹۳ به امضاء رسیده مسئله حاکمیت این چهار جزیره ایستاد#ایست پیش از امضاء پیمان صلح فیصله یافت#یاب\n","هیچ یک از دو طرف نحوه استفاده از پیمان پیشین را اعلام کرد#کن\n","\n","تشکیلات فلسطین نخستین بودجه رسم خود را اعلام کرد#کن\n","تشکیلات فلسطین پس از دو سال نخستین بودجه رسم خود را اعلام کرد#کن و قول داد برای از میان برداشتن فساد و پاسخگوئی بیشتر به مردم تلاش کند\n"]}],"source":["!head voa_persian_normalized.txt"]},{"cell_type":"markdown","metadata":{"id":"r9-rAN6e2tNx"},"source":["### Language Model"]},{"cell_type":"markdown","metadata":{"id":"AWdUYOwnsXKj"},"source":["Now we'll use `KenLM` to train an N-gram language model. an N-gram model calculates probability of N words being together.\n","\n","You can read more about N-gram [here](https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058).\n","\n","First, let's install download and build `KenLM`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":131575,"status":"ok","timestamp":1684081956233,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"TtuYeg5Gq2AW"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2023-05-14 16:54:01--  https://kheafield.com/code/kenlm.tar.gz\n","Resolving kheafield.com (kheafield.com)... 35.196.63.85\n","Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 491888 (480K) [application/x-gzip]\n","Saving to: ‘STDOUT’\n","\n","-                   100%[===================\u003e] 480.36K  1.62MB/s    in 0.3s    \n","\n","2023-05-14 16:54:01 (1.62 MB/s) - written to stdout [491888/491888]\n","\n","mkdir: cannot create directory ‘kenlm/build’: File exists\n","-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/kenlm/build\n","[ 38%] Built target kenlm_util\n","[ 41%] Built target probing_hash_table_benchmark\n","[ 46%] Built target kenlm_filter\n","[ 71%] Built target kenlm\n","[ 76%] Built target fragment\n","[ 76%] Built target query\n","[ 78%] Built target build_binary\n","[ 81%] Built target kenlm_benchmark\n","[ 83%] Built target filter\n","[ 92%] Built target kenlm_builder\n","[ 95%] Built target phrase_table_vocab\n","[ 97%] Built target lmplz\n","[100%] Built target count_ngrams\n"]}],"source":["!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz; mkdir kenlm/build; cd kenlm/build; cmake ..; make -j2"]},{"cell_type":"markdown","metadata":{"id":"-NB9vhHY3Wkt"},"source":["Now let's make a 5-gram model using "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29487,"status":"ok","timestamp":1684081985716,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"Wvr9XW2btIp-"},"outputs":[{"name":"stdout","output_type":"stream","text":["=== 1/5 Counting and sorting n-grams ===\n","Reading /content/voa_persian_normalized.txt\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Unigram tokens 7151925 types 105480\n","=== 2/5 Calculating and sorting adjusted counts ===\n","Chain sizes: 1:1265760 2:1062379968 3:1991962624 4:3187139840 5:4647912448\n","Statistics:\n","1 105480 D1=0.692744 D2=1.02088 D3+=1.36881\n","2 1273882 D1=0.753645 D2=1.09856 D3+=1.34093\n","3 3443025 D1=0.837135 D2=1.17753 D3+=1.39364\n","4 5019416 D1=0.905518 D2=1.28914 D3+=1.4375\n","5 5611302 D1=0.891822 D2=1.51475 D3+=1.61147\n","Memory estimate for binary LM:\n","type     MB\n","probing 321 assuming -p 1.5\n","probing 377 assuming -r models -p 1.5\n","trie    153 without quantization\n","trie     83 assuming -q 8 -b 8 quantization \n","trie    135 assuming -a 22 array pointer compression\n","trie     66 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n","=== 3/5 Calculating and sorting initial probabilities ===\n","Chain sizes: 1:1265760 2:20382112 3:68860500 4:120465984 5:157116456\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","####################################################################################################\n","=== 4/5 Calculating and writing order-interpolated probabilities ===\n","Chain sizes: 1:1265760 2:20382112 3:68860500 4:120465984 5:157116456\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","####################################################################################################\n","=== 5/5 Writing ARPA model ===\n","----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n","****************************************************************************************************\n","Name:lmplz\tVmPeak:10807668 kB\tVmRSS:29480 kB\tRSSMax:2062680 kB\tuser:19.9164\tsys:7.77465\tCPU:27.6911\treal:29.251\n"]}],"source":["!kenlm/build/bin/lmplz -o 5 \u003c\"voa_persian_normalized.txt\"\u003e \"voa_persian.arpa\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1684081985717,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"lHIy7Bo8FDAO"},"outputs":[{"name":"stdout","output_type":"stream","text":["\\data\\\n","ngram 1=105480\n","ngram 2=1273882\n","ngram 3=3443025\n","ngram 4=5019416\n","ngram 5=5611302\n","\n","\\1-grams:\n","-6.13853\t\u003cunk\u003e\t0\n","0\t\u003cs\u003e\t-1.5815215\n","-2.112993\t\u003c/s\u003e\t0\n","-3.5871985\tپیمان\t-0.5081966\n","-3.3047166\tصلح\t-0.5604726\n","-2.8914635\tبین\t-0.67792463\n","-3.3033433\tژاپن\t-0.5074516\n","-2.0037067\tو\t-0.8103022\n","-3.0975704\tروسیه\t-0.56394786\n","-3.6942556\tبنا\t-0.50759333\n","-2.0797186\tبه\t-1.0189861\n","-3.0476315\tگزارش\t-0.6365013\n"]}],"source":["!head -n 20 voa_persian.arpa"]},{"cell_type":"markdown","metadata":{"id":"gkcS4dFtujvo"},"source":["Now lets extract the list of words and sort them using their probabilities."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1684081985718,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"jSR4Cw6st1G7"},"outputs":[{"data":{"text/plain":["['\u003c/s\u003e',\n"," 'در',\n"," 'و',\n"," 'به',\n"," 'را',\n"," 'که',\n"," 'از',\n"," 'با',\n"," '#است',\n"," 'بود#باش',\n"," 'یک',\n"," 'برای',\n"," 'این',\n"," 'شد#شو',\n"," 'گفت#گو',\n"," 'خود',\n"," 'آن',\n"," 'کرد#کن',\n"," 'روز',\n"," 'نیز']"]},"execution_count":81,"metadata":{},"output_type":"execute_result"}],"source":["words = []\n","words_started = False\n","with open('voa_persian.arpa') as f:\n","    for line in f:\n","        line = line.strip()\n","        if not words_started:\n","            if line == r'\\1-grams:':\n","                words_started = True\n","        else:\n","            if line == r'\\2-grams:':\n","                words = words[:-1]\n","                break\n","            words.append(line.split())\n","words_sorted = sorted(words, key=lambda x: x[0])\n","words_total = [w[1] for w in words_sorted]\n","words_total.remove('\u003c/s\u003e')\n","words_total.insert(0, '\u003c/s\u003e')\n","words_total[:20]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15479,"status":"ok","timestamp":1684082001192,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"lxznD7kt3f_T"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting https://github.com/kpu/kenlm/archive/master.zip\n","  Using cached https://github.com/kpu/kenlm/archive/master.zip\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install https://github.com/kpu/kenlm/archive/master.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"eIeI7WM13nK4"},"outputs":[],"source":["import kenlm\n","\n","model = kenlm.Model('voa_persian.arpa')"]},{"cell_type":"markdown","metadata":{"id":"RepmrJXhzmjt"},"source":["Now we need a measure using our language model to measure how well our sentence fit together. Our model can measure the probability of a sentence using N-gram.\n","\n","This has a downside. the longer the sentence gets, the lower its' probability becomes. We don't want that. So we introduce `perplexity`. a measure which is normalized by the sentence's length. Lower perplexity means the semantics of our sentence fits better together.\n","\n","You can read more about perplexity [here](https://medium.com/nlplanet/two-minutes-nlp-perplexity-explained-with-simple-probabilities-6cdc46884584).\n","$$\n","\\begin{align}\n","PP(S) \u0026= 10 ^ {-\\frac{log(P(S))}{N}} \\\\\n","PP(S) \u0026= \\sqrt[N]{\\frac{1}{P(S)}} \\\\\n","PP(S) \u0026= \\sqrt[N]{\\frac{1}{P(W_1W_2...W_N)}} \\\\\n","PP(S) \u0026= \\sqrt[N]{\\prod_{i=1}^N{\\frac{1}{P(W_i|W_1W_2...W_{i-1})}}}\n","\\end{align}\n","$$\n","**Note**: `KenLM` score function return log10 probability of a sentence."]},{"cell_type":"markdown","metadata":{"id":"B_kXmYP4A_JF"},"source":["### Perplexity (10 Points)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"oPH4awk57yOZ"},"outputs":[],"source":["def perplexity(sentence: str):\n","    \"\"\"\n","    returns the perplexity of a sentence using model.score method\n","    Args:\n","      sentence: string of words\n","\n","    Returns:\n","      perplexity: 10^(-lop10p(sentence) / N)\n","    \"\"\"\n","    # help from : https://stackoverflow.com/questions/43841467/how-to-compute-perplexity-using-kenlm\n","    words = len(sentence.split()) + 1 # For \u003c/s\u003e\n","    return 10.0**(- model.score(sentence) / words)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"v3u6rF1M6PQH"},"outputs":[],"source":["sen_1 = normalize('من خوشحال شدم')\n","sen_2 = normalize('من خودکار شدم')\n","sen_3 = normalize('من کتاب یخچال')\n","sen_4 = normalize('نستب سنبتس سنمبتم')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1684082017211,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"Sv44dRB_6e4C"},"outputs":[{"name":"stdout","output_type":"stream","text":["من خوشحال شد#شو 137.01680134111533\n","من خودکار شد#شو 1222.8542216279307\n","من کتاب یخچال 7467.088563904955\n","نستب سنبتس سنمبتم 336919.6792991999\n"]}],"source":["print(sen_1, perplexity(sen_1))\n","print(sen_2, perplexity(sen_2))\n","print(sen_3, perplexity(sen_3))\n","print(sen_4, perplexity(sen_4))\n"]},{"cell_type":"markdown","metadata":{"id":"z_oHC5Vy7ypg"},"source":["## Reinforcement Learning"]},{"cell_type":"markdown","metadata":{"id":"M2VRX8HM72PX"},"source":["### Reward Function (10 Points)"]},{"cell_type":"markdown","metadata":{"id":"lX_ET9KYb4ov"},"source":["Reward function should give us a reward based on how the last word added to the sentence changed the meaning and how well it fits with the others."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"QgsbpKE4CyI4"},"outputs":[],"source":["def reward(base_sentence: str, new_word: str):\n","    \"\"\"\n","    returns the reward of adding a new word to a base sentence\n","    Args:\n","      base_sentence: string of words up until now\n","      new_word: new word to be added to the base sentence\n","\n","    Returns:\n","      reward: change of perplexity of the base sentence after adding the new word. positive reward means the new word is more likely to be added to the base sentence.\n","    \"\"\"\n","    # need it ti be closer to 1 so we will give it a - property for now\n","    if new_word == '' or new_word == ' ' : return 0\n","    if base_sentence == '' or base_sentence == ' ' : -(perplexity(new_word) - perplexity(base_sentence))\n","    return -(perplexity(base_sentence + \" \" + new_word) - perplexity(base_sentence))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1684082017213,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"8jk1iYSm73l9"},"outputs":[{"name":"stdout","output_type":"stream","text":["-690.0787915528637\n","-167.71077564865902\n","723.9550949735619\n","457.5602143706758\n","483.02565363284526\n","-4981.013565729618\n"]}],"source":["print(reward('', 'من'))\n","print(reward('من', 'خوشحال'))\n","print(reward('من خوشحال', 'شد#شو'))\n","print(reward('جنگ جهانی', 'اول'))\n","print(reward('جنگ جهانی', 'دوم'))\n","print(reward('جنگ جهانی', 'صورتی'))\n"]},{"cell_type":"markdown","metadata":{"id":"F1ga2l1WczI5"},"source":["Since we have to implement text generator using a tabular implementation, we have to assume that all that matters in a text is in a window of N words. It matches our language model of N-gram.\n","\n","We model it using MDP. the first state is `\u003cs\u003e` state. it has no text and 0 perplexity. The next state is $W_1$ state. We usually have a negative perplexity because no text has more meaning than a one word sentence. Next is $W_1W_2$ state until we reach $W_1W_2...W_N$ state, from then with our window assumption we go to $W_2W_3...W_{N+1}$ state and $W_3W_4...W_{N+2}$ and so on.\n","\n","First thing we notice is that our search space is **really** big. Each word choice has thousands of possibilites. We cannot model our search space using our normal Q Table.\n","Since our states are sequential and we need to find the best word using our current state, we can use `dict` in `dict` architecture.\n","\n","First we reduce the search space to the 10K most used words.\n","For faster computation, we use each word index for states."]},{"cell_type":"markdown","metadata":{"id":"TPRME20tA_JJ"},"source":["### Utility Functions (10 Points)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":977,"status":"ok","timestamp":1684082349590,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"yXXEro4V2T9f"},"outputs":[{"name":"stdout","output_type":"stream","text":["یک\n","10\n","مقام کارت رئیس .\n","[389, 2887, 23]\n","مقام یک برقرار محترم\n","[389, 10, 787, 3809]\n"]}],"source":["words = words_total[:10000]\n","# 0 index is for \u003c/s\u003e which means end of the sentence.\n","indexes = dict()\n","for i, w in enumerate(words):\n","    indexes[w] = i\n","\n","\n","def index_to_word(index: int):\n","    \"\"\"\n","    returns the word of a given index\n","    Args:\n","        index: index of the word\n","\n","    Returns:\n","        word: word of the given index. '.' if the index is 0 (end of sentence or \u003c/s\u003e)\n","    \"\"\"\n","    if index is None : return \"\"\n","    if index \u003e len(words_total) : raise Exception(\"index out of bound\")\n","    if index == 0 : return '.'\n","    return words[index]\n","\n","\n","def word_to_index(word: str):\n","    \"\"\"\n","    returns the index of a given word\n","    Args:\n","        word: word of the given index. word should be normalized.\n","\n","    Returns:\n","        index: index of the word. -1 if the word is not in the vocabulary\n","    \"\"\"\n","    normalize(word)\n","    if word not in indexes : return -1\n","    if word == '.' : return 0\n","    return indexes[word]\n","\n","\n","def state_to_sentence(state: list[int]):\n","    \"\"\"\n","    returns the sentence of a given state\n","    Args:\n","        state: list of indexes of words\n","\n","    Returns:\n","        sentence: string of words. '.' when the state is 0 (end of sentence or \u003c/s\u003e)\n","    \"\"\"\n","    return \" \".join([index_to_word(x) for x in state])\n","\n","def sentence_to_state(sentence: str):\n","    \"\"\"\n","    returns the state of a given sentence\n","    Args:\n","        sentence: string of words. sentence should be normalized.\n","\n","    Returns:\n","        state: list of indexes of words. no need to add the index of \u003c/s\u003e (end of sentence) to the state\n","    \"\"\"\n","    sentence = normalize(sentence)\n","    if sentence[-1] == '\\s' or  sentence[-1] == '.' :  sentence = sentence[:-1]\n","    return [word_to_index(x) for x in sentence.split()]\n","\n","\n","print(index_to_word(10))\n","print(word_to_index('یک'))\n","print(state_to_sentence([390, 2884, 24, 0]))\n","print(sentence_to_state('من خوشحال هستم'))\n","print(state_to_sentence([390, 10, 791, 3816]))\n","print(sentence_to_state('من یک کتاب خریدم'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1684082017214,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"l5Jzz59DziLv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Q[من] 10\n","Q[من, خوشحال] 20\n","Q[من, خوشحال, هستم] 25\n"]},{"data":{"text/plain":["{389: (10,\n","  {2887: (20, {-1: (25, {0: (0, {})})}),\n","   10: (5, {787: (15, {-1: (10, {})}), 479: (15, {-1: (8, {})})})}),\n"," 2172: (10,\n","  {2887: (20, {7602: (7, {0: (0, {})})}),\n","   27: (5, {787: (15, {-1: (11, {})})})})}"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["# example Q Table\n","q_table = {\n","    word_to_index('من'): (10, {\n","        word_to_index('خوشحال'): (20, {\n","            word_to_index('هستم'): (25, {\n","                0: (0, {}),\n","            }),\n","        }),\n","        word_to_index('یک'): (5, {\n","            word_to_index('کتاب'): (15, {\n","                word_to_index('خریدم'): (10, {}),\n","            }),\n","            word_to_index('گل'): (15, {\n","                word_to_index('دیدم'): (8, {}),\n","            }),\n","        })\n","    }),\n","    word_to_index('تو'): (10, {\n","        word_to_index('خوشحال'): (20, {\n","            word_to_index('هستی'): (7, {\n","                0: (0, {}),\n","            }),\n","        }),\n","        word_to_index('دو'): (5, {\n","            word_to_index('کتاب'): (15, {\n","                word_to_index('خریدی'): (11, {}),\n","            }),\n","        })\n","    }),\n","}\n","print('Q[من]', q_table[word_to_index('من')][0])\n","print('Q[من, خوشحال]', q_table[word_to_index('من')]\n","      [1][word_to_index('خوشحال')][0])\n","print('Q[من, خوشحال, هستم]', q_table[word_to_index('من')][1]\n","      [word_to_index('خوشحال')][1][word_to_index('هستم')][0])\n","q_table\n"]},{"cell_type":"markdown","metadata":{"id":"rW2BjNOrA_JL"},"source":["### Hyperparameters\n","You can change these parameters to get better results."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"t3RDh4mzyG2k"},"outputs":[],"source":["q_table = dict()\n","alpha = 0.8\n","gamma = 0.95\n","state_N = 6\n","N = 75"]},{"cell_type":"markdown","metadata":{"id":"kWJYocVWA_JL"},"source":["### Q-Learning Utility Functions (50 Points)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dh9HClfJDQVT"},"outputs":[],"source":["import bisect\n","import random\n","from random import choices\n","\n","weights = [1 for i in range(10000)]\n","index = [i for i in range(10000)]\n","total_weights = sum(weights)\n","normalized_weights = [i / total_weights for i in weights]\n","\n","def random_index():\n","    \"\"\"\n","    returns a random index based on the weights\n","\n","    Returns:\n","        index: index of the word\n","    \"\"\"\n","    return choices(index ,normalized_weights)[0]\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dvtoh1fME_e7"},"outputs":[],"source":["import math\n","def q_table_max_find(q_table: dict[int, tuple[int, dict]], state: list[int]):\n","    \"\"\"\n","    returns the index of the word with the maximum Q value in the given state. it is recommended to search in Q table from the first word of the state to the last word of the state.\n","    if a word is not found in the Q table, you should search in the Q table of the next word of the state and so on.\n","    so if we don't have Q[W_1W_2...W_N], we search for Q[W_2W_3...W_N] and so on until Q[W_N]. if we don't have Q[W_N], we should return a random index.\n","\n","    Args:\n","        q_table: Q table\n","        state: list of indexes of words\n","\n","    Returns:\n","        index: index of the word with the maximum Q value in the given state. random index if the state is not in the Q table.\n","    \"\"\"\n","    max_index , max_val = None ,None\n","    for i in range(len(state)):\n","      if state[i] not in q_table : continue\n","      val ,next = q_table[state[i]]\n","      counter = i + 1\n","      failed = False\n","      while counter \u003c len(state) and not failed :\n","        if state[counter] not in next: failed = True\n","        else : val ,next = next[state[counter]]\n","        counter += 1\n","      if failed : continue\n","      for index in next :\n","        if max_val == None or max_val \u003c= next[index][0] :\n","          max_index = index  \n","          max_val = next[index][0]\n","      return random_index() if max_index is None else max_index\n","    \n","def q_table_update(q_table: dict[int, tuple[int, dict]], state: list[int]):\n","    \"\"\"\n","    updates the Q table based on the given state. update the Q[W_1W_2...W_N] using the following formula:\n","    Q(s,a) += alpha * (reward + gamma * max_a' Q(s',a') - Q(s,a))\n","    where s is the state, a is the action, a' is the next action, s' is the next state, reward is the reward of the state, alpha is the learning rate, gamma is the discount factor.\n","    then update the Q[W_1W_2...W_{N-1}] and so on until Q[W_1].\n","    \n","    Args:\n","        q_table: Q table\n","        state: list of indexes of words\n","    \"\"\"\n","    for index in range(len(state)):\n","      i = len(state) - index - 1  # for L(state) - 1 -\u003e 0 \n","      Q = q_table\n","      for j in range(i):\n","        if state[j] not in Q : Q[state[j]] = (0 , dict())\n","        val ,Q = Q[state[j]]\n","      if state[i] not in Q : Q[state[i]] = (0 , dict())\n","      R = Q[state[i]][0] + alpha * (reward(state_to_sentence(state[:i]) , index_to_word(state[i])) + gamma * max([Q[i][0] for i in Q]) - Q[state[i]][0])\n","      Q[state[i]] = (R , Q[state[i]][1])\n"]},{"cell_type":"markdown","metadata":{"id":"wijOOCSFA_JN"},"source":["### Training Loop (10 Points)\n","Since search space is really big, we can let our model train for an hour or two and get a good result."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":9746,"status":"error","timestamp":1684082688875,"user":{"displayName":"Ali Aghayari","userId":"02310680965711042070"},"user_tz":-210},"id":"IcLjORGcFDEM","outputId":"eaf7d813-f1af-48c3-d5f9-fefe3454048f"},"outputs":[{"name":"stderr","output_type":"stream","text":[" 20%|██        | 807/4000 [03:37\u003c14:21,  3.71it/s]\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-68-a4a37a45c30c\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 4\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u003c\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0;31m# TODO: random action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 9\u001b[0;31m       \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m       \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m\u003cipython-input-50-ce879475a5c6\u003e\u001b[0m in \u001b[0;36mrandom_index\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"\n\u001b[0;32m---\u003e 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mnormalized_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/lib/python3.10/random.py\u001b[0m in \u001b[0;36mchoices\u001b[0;34m(self, population, weights, cum_weights, k)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_repeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 521\u001b[0;31m                 \u001b[0mcum_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_accumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["episodes = 12000\n","epsilon = 1\n","episode_N = 75\n","for ep in tqdm(range(episodes)):\n","  state = []\n","  for i in range(episode_N):\n","    if random.random() \u003c epsilon:\n","      # TODO: random action\n","      action = random_index()\n","      state.append(action)\n","    else:\n","      # TODO: greedy action with Q table max find\n","      action = q_table_max_find(q_table , state)\n","      state.append(action)\n","    # to avoid infinite loop\n","    if len(state) \u003e 1 and state[-1] == state[-2]:\n","      break\n","    q_table_update(q_table, state)\n","    if len(state) \u003e state_N:\n","      state = state[1:]\n","    if state[-1] == 0:\n","      break\n","  epsilon *= 0.99975"]},{"cell_type":"markdown","metadata":{"id":"nV6oTXKOA_JN"},"source":["### Testing (10 Points)\n","This will be the final output of our model. score will be based on how well the output fits with the corpus. Generated sentences should have some meaning in the neighborhood of each word."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GSpB15EdFKjm"},"outputs":[],"source":["def get_result(state, steps=75):\n","    for i in range(steps):\n","        state.append(q_table_max_find(q_table, state))\n","        if state[-1] == 0:\n","            break\n","        if len(state) \u003e state_N:\n","            state = state[1:]\n","        yield state[-1]\n","\n","state = sentence_to_state('ما')\n","print('ما', end=' ')\n","for s in get_result(state):\n","    print(words[s], end=' ')\n","print()\n","state = sentence_to_state('یک')\n","print('یک', end=' ')\n","for s in get_result(state):\n","    print(words[s], end=' ')\n","print()\n","state = sentence_to_state('ایران')\n","print('ایران', end=' ')\n","for s in get_result(state):\n","    print(words[s], end=' ')\n","print()"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"vscode":{"interpreter":{"hash":"026f2bef8fb7be59296f2f39e2043bb013bc567dc5026fb77125b1034979614d"}}},"nbformat":4,"nbformat_minor":0}